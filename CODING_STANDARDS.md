# üìè Gu√≠a de Est√°ndares de C√≥digo
# Proyecto BMW Pricing - Data Engineering & Analysis

**Versi√≥n:** 1.0  
**Fecha:** Noviembre 2025  
**Prop√≥sito:** Normalizar el c√≥digo entre todos los integrantes para facilitar la integraci√≥n final

---

<a id="tabla-de-contenidos"></a>
## üìã Tabla de Contenidos

- [Convenciones Generales](#convenciones-generales)
- [Nombres de Variables](#nombres-de-variables)
- [Estructura del Notebook](#estructura-del-notebook)
- [Librer√≠as y Imports](#librer√≠as-y-imports)
- [Carga de Datos](#carga-de-datos)
- [An√°lisis Exploratorio](#an√°lisis-exploratorio)
- [Limpieza de Duplicados](#limpieza-de-duplicados)
- [Tratamiento de Valores Nulos](#tratamiento-de-valores-nulos)
- [Detecci√≥n de Outliers](#detecci√≥n-de-outliers)
- [Feature Engineering](#feature-engineering)
- [Documentaci√≥n y Comentarios](#documentaci√≥n-y-comentarios)
- [Visualizaciones](#visualizaciones)
- [Funciones Reutilizables](#funciones-reutilizables)
- [Checklist Final](#checklist-final)

---

<a id="convenciones-generales"></a>
## Convenciones Generales

### Reglas B√°sicas

```python
# ‚úÖ HACER
- Usar ingl√©s para nombres de columnas del dataset original
- Usar snake_case para variables: mi_variable_nombre
- Usar CamelCase para clases: MiClase
- Nombres descriptivos y claros
- Comentarios en espa√±ol (para claridad del equipo)
- M√°ximo 79 caracteres por l√≠nea (PEP 8)

# ‚ùå NO HACER
- Nombres gen√©ricos: x, y, temp, data
- CamelCase para variables: MiVariable
- Nombres en espa√±ol para variables del dataset: precio, kilometraje
- L√≠neas de m√°s de 100 caracteres
```

### Estilo de C√≥digo

```python
# ‚úÖ Espaciado correcto
df_clean = df[df['price'] > 0]
resultado = funcion(parametro1, parametro2)

# ‚ùå Sin espacios o mal espaciado
df_clean=df[df['price']>0]
resultado = funcion( parametro1,parametro2 )
```

---

<a id="nombres-de-variables"></a>
## Nombres de Variables

### 1. DataFrame Principal

```python
# ‚úÖ USAR SIEMPRE
df                  # DataFrame original despu√©s de la carga
df_clean            # DataFrame despu√©s de limpieza completa
df_encoded          # DataFrame despu√©s de encoding
df_scaled           # DataFrame despu√©s de scaling
df_final            # DataFrame listo para guardar

# ‚ùå NO USAR
datos, data, bmw_data, dataset, df_limpio, clean_df
```

### 2. DataFrames Temporales

```python
# ‚úÖ Para operaciones intermedias
df_temp             # DataFrame temporal
df_no_duplicates    # Despu√©s de eliminar duplicados
df_no_nulls         # Despu√©s de eliminar nulos
df_no_outliers      # Despu√©s de eliminar outliers

# ‚ùå NO USAR
temp, tmp, df1, df2, df3
```

### 3. Variables de Columnas

```python
# ‚úÖ Usar los nombres originales del dataset
'price'             # No 'precio'
'mileage'           # No 'kilometraje'
'year'              # No 'ano' o 'anio'
'model'             # No 'modelo'
'brand'             # No 'marca'
'fuel_type'         # No 'tipo_combustible'

# ‚úÖ Para listas de columnas
numeric_cols        # Columnas num√©ricas
categorical_cols    # Columnas categ√≥ricas
cols_with_nulls     # Columnas con valores nulos
cols_to_drop        # Columnas a eliminar

# ‚ùå NO USAR
numericas, categoricas, columnas_numericas
```

### 4. Variables Num√©ricas

```python
# ‚úÖ Nombres descriptivos
n_rows              # N√∫mero de filas
n_cols              # N√∫mero de columnas
n_duplicates        # N√∫mero de duplicados
n_nulls             # N√∫mero de valores nulos
retention_rate      # Tasa de retenci√≥n (%)
missing_threshold   # Umbral de valores faltantes

# ‚ùå NO USAR
num, cantidad, total, x, n
```

### 5. Variables de Umbral/Threshold

```python
# ‚úÖ Para umbrales y l√≠mites
outlier_threshold = 1.5        # Factor IQR para outliers
null_threshold = 0.5           # M√°ximo % de nulos permitido
correlation_threshold = 0.8    # Correlaci√≥n alta

# ‚ùå NO USAR
umbral, limite, threshold_val, th
```

**[‚¨Ü back to top](#tabla-de-contenidos)**

---

<a id="estructura-del-notebook"></a>
## Estructura del Notebook

### Plantilla Obligatoria

Todos los notebooks individuales DEBEN seguir esta estructura:

```python
"""
====================================================================
LIMPIEZA Y PREPROCESAMIENTO - DATASET BMW PRICING
====================================================================
Autor: [TU NOMBRE COMPLETO]
Fecha: [FECHA]
Descripci√≥n: Proceso completo de limpieza de datos para preparar
             el dataset BMW para modelado predictivo de precios.
====================================================================
"""

# ========================================
# 1. IMPORTS Y CONFIGURACI√ìN
# ========================================

# ========================================
# 2. CARGA DE DATOS
# ========================================

# ========================================
# 3. AN√ÅLISIS EXPLORATORIO INICIAL
# ========================================
# 3.1 Informaci√≥n General
# 3.2 Estad√≠sticas Descriptivas
# 3.3 An√°lisis de Tipos de Datos

# ========================================
# 4. AN√ÅLISIS DE CALIDAD DE DATOS
# ========================================
# 4.1 Duplicados
# 4.2 Valores Nulos
# 4.3 Valores Inconsistentes

# ========================================
# 5. LIMPIEZA DE DUPLICADOS
# ========================================

# ========================================
# 6. TRATAMIENTO DE VALORES NULOS
# ========================================
# 6.1 An√°lisis de Patrones
# 6.2 Estrategia de Imputaci√≥n
# 6.3 Implementaci√≥n

# ========================================
# 7. DETECCI√ìN Y TRATAMIENTO DE OUTLIERS
# ========================================
# 7.1 Variables Num√©ricas
# 7.2 Visualizaci√≥n
# 7.3 Tratamiento

# ========================================
# 8. TRANSFORMACI√ìN DE VARIABLES
# ========================================
# 8.1 Variables Categ√≥ricas
# 8.2 Variables Num√©ricas

# ========================================
# 9. FEATURE ENGINEERING
# ========================================

# ========================================
# 10. VALIDACI√ìN FINAL
# ========================================

# ========================================
# 11. EXPORTACI√ìN DE DATOS LIMPIOS
# ========================================

# ========================================
# 12. RESUMEN Y CONCLUSIONES
# ========================================
```

**[‚¨Ü back to top](#tabla-de-contenidos)**

---

<a id="librer√≠as-y-imports"></a>
## Librer√≠as y Imports

### 1. Orden y Agrupaci√≥n

```python
# ========================================
# 1. IMPORTS Y CONFIGURACI√ìN
# ========================================

# Librer√≠as est√°ndar de Python
import os
import sys
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Librer√≠as de an√°lisis de datos
import pandas as pd
import numpy as np

# Librer√≠as de visualizaci√≥n
import matplotlib.pyplot as plt
import seaborn as sns

# Librer√≠as de machine learning
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from scipy import stats
from scipy.stats import zscore

# Configuraci√≥n de visualizaci√≥n
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 100)
pd.set_option('display.float_format', '{:.2f}'.format)

# Configuraci√≥n de reproducibilidad
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)
```

### 2. Configuraci√≥n de Gr√°ficos

```python
# ‚úÖ Configuraci√≥n est√°ndar para todos
plt.rcParams['figure.figsize'] = (12, 6)
plt.rcParams['font.size'] = 10
plt.rcParams['axes.titlesize'] = 12
plt.rcParams['axes.labelsize'] = 11
```

**[‚¨Ü back to top](#tabla-de-contenidos)**

---

<a id="carga-de-datos"></a>
## Carga de Datos

### 1. C√≥digo Est√°ndar

```python
# ========================================
# 2. CARGA DE DATOS
# ========================================

# Ruta al archivo de datos
DATA_PATH = 'data/raw/bmw_pricing_v3.csv'

# Cargar dataset
print("Cargando dataset...")
df = pd.read_csv(DATA_PATH)

# Informaci√≥n b√°sica
print(f"‚úì Dataset cargado exitosamente")
print(f"  - Filas: {df.shape[0]:,}")
print(f"  - Columnas: {df.shape[1]}")
print(f"  - Memoria: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
```

### 2. Crear Copia de Seguridad

```python
# ‚úÖ SIEMPRE crear backup del original
df_original = df.copy()
print(f"‚úì Backup creado: {df_original.shape[0]:,} registros")
```

---

<a id="an√°lisis-exploratorio"></a>
## An√°lisis Exploratorio

### 1. Informaci√≥n General
```python
# ========================================
# 3. AN√ÅLISIS EXPLORATORIO INICIAL
# ========================================

# 3.1 Informaci√≥n General
print("=" * 60)
print("INFORMACI√ìN GENERAL DEL DATASET")
print("=" * 60)

print("\nüìä Estructura del Dataset:")
df.info()

print("\nüìà Primeras 5 filas:")
display(df.head())

print("\nüìâ √öltimas 5 filas:")
display(df.tail())
```

### 2. Estad√≠sticas Descriptivas
```python
# 3.2 Estad√≠sticas Descriptivas
print("\n" + "=" * 60)
print("ESTAD√çSTICAS DESCRIPTIVAS")
print("=" * 60)

print("\nüî¢ Variables Num√©ricas:")
display(df.describe())

print("\nüìù Variables Categ√≥ricas:")
display(df.describe(include='object'))
```

### 3. An√°lisis de Tipos
```python
# 3.3 An√°lisis de Tipos de Datos
print("\n" + "=" * 60)
print("AN√ÅLISIS DE TIPOS DE DATOS")
print("=" * 60)

# Identificar tipos de columnas
numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
categorical_cols = df.select_dtypes(include=['object']).columns.tolist()

print(f"\n‚úì Variables num√©ricas ({len(numeric_cols)}):")
for col in numeric_cols:
    print(f"  - {col}: {df[col].dtype}")

print(f"\n‚úì Variables categ√≥ricas ({len(categorical_cols)}):")
for col in categorical_cols:
    n_unique = df[col].nunique()
    print(f"  - {col}: {n_unique} valores √∫nicos")
```

**[‚¨Ü back to top](#tabla-de-contenidos)**

---

<a id="limpieza-de-duplicados"></a>
## Limpieza de Duplicados

### 1. C√≥digo Est√°ndar
```python
# ========================================
# 5. LIMPIEZA DE DUPLICADOS
# ========================================

print("=" * 60)
print("AN√ÅLISIS Y ELIMINACI√ìN DE DUPLICADOS")
print("=" * 60)

# An√°lisis de duplicados
n_duplicates = df.duplicated().sum()
print(f"\nüìä Duplicados completos: {n_duplicates} ({n_duplicates/len(df)*100:.2f}%)")

if n_duplicates > 0:
    print("\nEjemplos de duplicados:")
    display(df[df.duplicated(keep=False)].head(10))

    # Eliminar duplicados
    df_clean = df.drop_duplicates(keep='first')

    # Reporte
    n_removed = len(df) - len(df_clean)
    print(f"\n‚úì Duplicados eliminados: {n_removed}")
    print(f"  - Registros antes: {len(df):,}")
    print(f"  - Registros despu√©s: {len(df_clean):,}")
    print(f"  - Retenci√≥n: {len(df_clean)/len(df)*100:.2f}%")

    df = df_clean
else:
    print("\n‚úì No se encontraron duplicados completos")

# Duplicados por columnas clave (si aplica)
key_cols = ['model', 'year', 'price', 'mileage']
n_duplicates_key = df.duplicated(subset=key_cols).sum()

print(f"\nüìä Duplicados por columnas clave {key_cols}: {n_duplicates_key}")

if n_duplicates_key > 0:
    print(f"\n‚ö†Ô∏è Encontrados {n_duplicates_key} duplicados potenciales")
    print("An√°lisis manual requerido")
```

**[‚¨Ü back to top](#tabla-de-contenidos)**

---

<a id="tratamiento-de-valores-nulos"></a>
## Tratamiento de Valores Nulos

### 1. An√°lisis de Nulos
```python
# ========================================
# 6. TRATAMIENTO DE VALORES NULOS
# ========================================

print("=" * 60)
print("AN√ÅLISIS DE VALORES NULOS")
print("=" * 60)

# 6.1 An√°lisis de Patrones
# Contar nulos por columna
null_counts = df.isnull().sum()
null_percentages = (null_counts / len(df) * 100).round(2)

# DataFrame de resumen
null_summary = pd.DataFrame({
    'Columna': null_counts.index,
    'Valores_Nulos': null_counts.values,
    'Porcentaje': null_percentages.values
}).sort_values('Valores_Nulos', ascending=False)

# Filtrar solo columnas con nulos
null_summary = null_summary[null_summary['Valores_Nulos'] > 0]

if len(null_summary) > 0:
    print(f"\nüìä Columnas con valores nulos ({len(null_summary)}):")
    display(null_summary)

    # Visualizaci√≥n
    plt.figure(figsize=(12, 6))
    sns.heatmap(df.isnull(), cbar=False, yticklabels=False, cmap='viridis')
    plt.title('Mapa de Valores Nulos')
    plt.xlabel('Columnas')
    plt.tight_layout()
    plt.show()
else:
    print("\n‚úì No se encontraron valores nulos")
```

### 2. Estrategia de Imputaci√≥n
```python
# 6.2 Estrategia de Imputaci√≥n

print("\n" + "=" * 60)
print("ESTRATEGIA DE IMPUTACI√ìN")
print("=" * 60)

# Definir umbrales
null_threshold_drop = 50  # % de nulos para eliminar columna
null_threshold_impute = 50  # % de nulos para imputar

# Columnas a eliminar (>50% nulos)
cols_to_drop = null_summary[
    null_summary['Porcentaje'] > null_threshold_drop
]['Columna'].tolist()

# Columnas a imputar (<50% nulos)
cols_to_impute = null_summary[
    null_summary['Porcentaje'] <= null_threshold_impute
]['Columna'].tolist()

print(f"\nüìâ Columnas a eliminar (>{null_threshold_drop}% nulos): {len(cols_to_drop)}")
for col in cols_to_drop:
    pct = null_summary[null_summary['Columna'] == col]['Porcentaje'].values[0]
    print(f"  - {col}: {pct}% nulos")

print(f"\nüîß Columnas a imputar (<={null_threshold_impute}% nulos): {len(cols_to_impute)}")
for col in cols_to_impute:
    pct = null_summary[null_summary['Columna'] == col]['Porcentaje'].values[0]
    print(f"  - {col}: {pct}% nulos")
```

### 3. Implementaci√≥n
```python
# 6.3 Implementaci√≥n

# Eliminar columnas con muchos nulos
if len(cols_to_drop) > 0:
    df = df.drop(columns=cols_to_drop)
    print(f"\n‚úì Eliminadas {len(cols_to_drop)} columnas")

# Imputar valores nulos
for col in cols_to_impute:
    if col in numeric_cols:
        # Variables num√©ricas: mediana
        median_value = df[col].median()
        df[col].fillna(median_value, inplace=True)
        print(f"‚úì {col}: imputado con mediana ({median_value:.2f})")
    else:
        # Variables categ√≥ricas: moda
        mode_value = df[col].mode()[0]
        df[col].fillna(mode_value, inplace=True)
        print(f"‚úì {col}: imputado con moda ('{mode_value}')")

# Verificar que no quedan nulos
remaining_nulls = df.isnull().sum().sum()
print(f"\n‚úì Valores nulos restantes: {remaining_nulls}")
```

**[‚¨Ü back to top](#tabla-de-contenidos)**

---

<a id="detecci√≥n-de-outliers"></a>
## Detecci√≥n de Outliers

### 1. M√©todo IQR
```python
# ========================================
# 7. DETECCI√ìN Y TRATAMIENTO DE OUTLIERS
# ========================================

print("=" * 60)
print("DETECCI√ìN DE OUTLIERS")
print("=" * 60)

# 7.1 Variables Num√©ricas

# Funci√≥n para detectar outliers con IQR
def detect_outliers_iqr(df: pd.DataFrame, column: str, factor: float = 1.5) -> tuple:
    """
    Detecta outliers usando el m√©todo IQR.

    Args:
    -----------
    df : DataFrame con los datos
    column : Nombre de la columna
    factor : Factor multiplicador de IQR (default=1.5)

    Returns:
    --------
        (lower_bound, upper_bound, n_outliers, outlier_indices)
    """
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1

    lower_bound = Q1 - factor * IQR
    upper_bound = Q3 + factor * IQR

    outlier_mask = (df[column] < lower_bound) | (df[column] > upper_bound)
    n_outliers = outlier_mask.sum()
    outlier_indices = df[outlier_mask].index

    return lower_bound, upper_bound, n_outliers, outlier_indices

# Analizar cada variable num√©rica
outlier_summary = []

for col in numeric_cols:
    lower, upper, n_outliers, _ = detect_outliers_iqr(df, col)
    pct_outliers = (n_outliers / len(df) * 100)

    outlier_summary.append({
        'Columna': col,
        'Limite_Inferior': lower,
        'Limite_Superior': upper,
        'N_Outliers': n_outliers,
        'Porcentaje': pct_outliers
    })

outlier_df = pd.DataFrame(outlier_summary)
outlier_df = outlier_df[outlier_df['N_Outliers'] > 0]

print(f"\nüìä Variables con outliers:")
display(outlier_df)
```

### 2. Visualizaci√≥n
```python
# 7.2 Visualizaci√≥n

# Boxplots para variables con outliers
if len(outlier_df) > 0:
    n_plots = len(outlier_df)
    n_cols = 3
    n_rows = (n_plots + n_cols - 1) // n_cols

    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))
    axes = axes.flatten() if n_plots > 1 else [axes]

    for idx, col in enumerate(outlier_df['Columna']):
        ax = axes[idx]
        df.boxplot(column=col, ax=ax)
        ax.set_title(f'Boxplot: {col}')
        ax.set_ylabel('Valor')

    # Ocultar subplots vac√≠os
    for idx in range(n_plots, len(axes)):
        axes[idx].set_visible(False)

    plt.tight_layout()
    plt.show()
```

### 3. Tratamiento
```python
# 7.3 Tratamiento

print("\n" + "=" * 60)
print("TRATAMIENTO DE OUTLIERS")
print("=" * 60)

# Estrategia: Eliminar outliers extremos en price
col_to_treat = 'price'

if col_to_treat in df.columns:
    lower, upper, n_outliers, outlier_idx = detect_outliers_iqr(df, col_to_treat, factor=1.5)

    print(f"\nüìä Variable: {col_to_treat}")
    print(f"  - L√≠mite inferior: {lower:.2f}")
    print(f"  - L√≠mite superior: {upper:.2f}")
    print(f"  - Outliers detectados: {n_outliers} ({n_outliers/len(df)*100:.2f}%)")

    # Eliminar outliers
    df_no_outliers = df[(df[col_to_treat] >= lower) & (df[col_to_treat] <= upper)]

    n_removed = len(df) - len(df_no_outliers)
    print(f"\n‚úì Outliers eliminados: {n_removed}")
    print(f"  - Registros antes: {len(df):,}")
    print(f"  - Registros despu√©s: {len(df_no_outliers):,}")
    print(f"  - Retenci√≥n: {len(df_no_outliers)/len(df)*100:.2f}%")

    df = df_no_outliers
```

**[‚¨Ü back to top](#tabla-de-contenidos)**

---

<a id="feature-engineering"></a>
## Feature Engineering

### 1. Encoding de Variables Categ√≥ricas
```python
# ========================================
# 9. FEATURE ENGINEERING
# ========================================

print("=" * 60)
print("FEATURE ENGINEERING")
print("=" * 60)

# 9.1 Encoding de Variables Categ√≥ricas

# Identificar variables categ√≥ricas
categorical_cols = df.select_dtypes(include=['object']).columns.tolist()

print(f"\nüìù Variables categ√≥ricas a codificar: {len(categorical_cols)}")

# One-Hot Encoding
df_encoded = pd.get_dummies(
    df, 
    columns=categorical_cols, 
    drop_first=True,  # Evitar multicolinealidad
    dtype=int
)

print(f"\n‚úì Encoding completado")
print(f"  - Columnas antes: {df.shape[1]}")
print(f"  - Columnas despu√©s: {df_encoded.shape[1]}")
print(f"  - Nuevas columnas: {df_encoded.shape[1] - df.shape[1]}")

df = df_encoded
```

### 2. Scaling de Variables Num√©ricas
```python
# 9.2 Scaling de Variables Num√©ricas

# Identificar columnas num√©ricas (sin las dummies)
numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()

# Excluir columnas dummy (valores 0 y 1)
numeric_cols_to_scale = [
    col for col in numeric_cols 
    if not df[col].isin([0, 1]).all()
]

print(f"\nüî¢ Variables num√©ricas a escalar: {len(numeric_cols_to_scale)}")
for col in numeric_cols_to_scale:
    print(f"  - {col}")

# MinMaxScaler (0-1)
scaler = MinMaxScaler()
df_scaled = df.copy()
df_scaled[numeric_cols_to_scale] = scaler.fit_transform(df[numeric_cols_to_scale])

print(f"\n‚úì Scaling completado (MinMaxScaler)")
print(f"  - Rango: [0, 1]")

df = df_scaled
```

**[‚¨Ü back to top](#tabla-de-contenidos)**

---

<a id="documentaci√≥n-y-comentarios"></a>
## Documentaci√≥n y Comentarios

### 1. Estilo de Comentarios
```python
# ‚úÖ Comentarios claros y descriptivos

# An√°lisis de distribuci√≥n de precios
# Se observa sesgo positivo, con concentraci√≥n en rango medio

# ‚ùå Comentarios vagos
# Haciendo cosas
# An√°lisis
```

### 2. Docstrings para Funciones
```python
# ‚úÖ Docstring completo
def eliminar_outliers(df: pd.DataFrame, column: str, factor: float = 1.5) -> pd.DataFrame:
    """
    Elimina outliers de una columna usando el m√©todo IQR.

    Args:
    -----------
    df : DataFrame con los datos
    column : Nombre de la columna a procesar
    factor : Factor multiplicador del IQR para definir l√≠mites

    Returns:
    --------
        DataFrame sin los outliers de la columna especificada

    Example:
    --------
    >>> df_clean = eliminar_outliers(df, 'price', factor=1.5)
    >>> print(f"Registros removidos: {len(df) - len(df_clean)}")
    """
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1

    lower = Q1 - factor * IQR
    upper = Q3 + factor * IQR

    return df[(df[column] >= lower) & (df[column] <= upper)]
```

### 3. Markdown en Celdas
```markdown
# ‚úÖ Celdas markdown bien estructuradas

## üîç An√°lisis de Duplicados

**Objetivo:** Identificar y eliminar registros duplicados

**Estrategia:**
1. Detectar duplicados completos (todas las columnas)
2. Detectar duplicados por columnas clave (model, year, price)
3. Eliminar manteniendo el primer registro

**Criterios:**
- Duplicado completo: todas las columnas iguales
- Duplicado clave: model + year + price iguales
```

**[‚¨Ü back to top](#tabla-de-contenidos)**

---

<a id="visualizaciones"></a>
## Visualizaciones

### 1. Estilo Est√°ndar
```python
# ‚úÖ Configuraci√≥n est√°ndar para gr√°ficos

plt.figure(figsize=(12, 6))

# Tu gr√°fico aqu√≠
plt.plot(x, y)

# Elementos obligatorios
plt.title('T√≠tulo Descriptivo del Gr√°fico', fontsize=14, fontweight='bold')
plt.xlabel('Etiqueta Eje X', fontsize=12)
plt.ylabel('Etiqueta Eje Y', fontsize=12)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

### 2. Paleta de Colores
```python
# ‚úÖ Usar paletas predefinidas
sns.set_palette("husl")
# O
sns.color_palette("viridis")

# ‚ùå Evitar colores arbitrarios
colors = ['#FF5733', '#123456', '#ABCDEF']
```

### 3. Ejemplos de Gr√°ficos Est√°ndar
```python
# Histograma
plt.figure(figsize=(12, 6))
plt.hist(df['price'], bins=50, edgecolor='black', alpha=0.7)
plt.title('Distribuci√≥n de Precios', fontsize=14)
plt.xlabel('Precio (‚Ç¨)', fontsize=12)
plt.ylabel('Frecuencia', fontsize=12)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Boxplot
plt.figure(figsize=(12, 6))
df.boxplot(column='mileage')
plt.title('Boxplot: Kilometraje', fontsize=14)
plt.ylabel('Kilometraje (km)', fontsize=12)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Heatmap de correlaci√≥n
plt.figure(figsize=(12, 10))
correlation_matrix = df[numeric_cols].corr()
sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', 
            square=True, linewidths=0.5)
plt.title('Matriz de Correlaci√≥n', fontsize=14)
plt.tight_layout()
plt.show()
```

**[‚¨Ü back to top](#tabla-de-contenidos)**

---

<a id="funciones-reutilizables"></a>
## Funciones Reutilizables

### Librer√≠a de Funciones Comunes
```python
# ========================================
# FUNCIONES AUXILIARES
# ========================================

def print_section(title: str):
    """
        Imprime un separador visual para secciones
    """
    print("\n" + "=" * 60)
    print(title)
    print("=" * 60)


def analyze_nulls(df: pd.DataFrame) -> pd.DataFrame:
    """
    Analiza valores nulos en el DataFrame

    Args:
    -----------
    df : DataFrame a analizar

    Returns:
    --------
        Resumen de valores nulos por columna
    """
    null_counts = df.isnull().sum()
    null_percentages = (null_counts / len(df) * 100).round(2)

    null_summary = pd.DataFrame({
        'Columna': null_counts.index,
        'Valores_Nulos': null_counts.values,
        'Porcentaje': null_percentages.values
    }).sort_values('Valores_Nulos', ascending=False)

    return null_summary[null_summary['Valores_Nulos'] > 0]


def detect_outliers_iqr(df: pd.DataFrame, column: str, factor: float = 1.5) -> tuple:
    """
    Detecta outliers usando el m√©todo IQR

    Args:
    -----------
    df : DataFrame con los datos
    column : Nombre de la columna
    factor : Factor multiplicador de IQR

    Returns:
    --------
        (lower_bound, upper_bound, n_outliers, outlier_indices)
    """
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1

    lower_bound = Q1 - factor * IQR
    upper_bound = Q3 + factor * IQR

    outlier_mask = (df[column] < lower_bound) | (df[column] > upper_bound)
    n_outliers = outlier_mask.sum()
    outlier_indices = df[outlier_mask].index

    return lower_bound, upper_bound, n_outliers, outlier_indices


def remove_outliers(df: pd.DataFrame, column: str, factor: float = 1.5) -> pd.DataFrame:
    """
    Elimina outliers de una columna usando IQR

    Args:
    -----------
    df : DataFrame con los datos
    column : Nombre de la columna
    factor : Factor multiplicador de IQR

    Returns:
    --------
        DataFrame sin outliers en la columna especificada
    """
    lower, upper, _, _ = detect_outliers_iqr(df, column, factor)
    return df[(df[column] >= lower) & (df[column] <= upper)]


def compare_distributions(df_before: pd.DataFrame, df_after: pd.DataFrame, column: str) -> None:
    """
    Compara distribuciones antes y despu√©s de limpieza

    Args:
    -----------
    df_before : DataFrame original
    df_after : DataFrame despu√©s de limpieza
    column : Columna a comparar
    """
    fig, axes = plt.subplots(1, 2, figsize=(15, 5))

    # Antes
    axes[0].hist(df_before[column], bins=50, edgecolor='black', alpha=0.7)
    axes[0].set_title(f'Antes: {column}')
    axes[0].set_xlabel(column)
    axes[0].set_ylabel('Frecuencia')
    axes[0].grid(True, alpha=0.3)

    # Despu√©s
    axes[1].hist(df_after[column], bins=50, edgecolor='black', alpha=0.7, color='green')
    axes[1].set_title(f'Despu√©s: {column}')
    axes[1].set_xlabel(column)
    axes[1].set_ylabel('Frecuencia')
    axes[1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()


def calculate_retention_rate(df_original: pd.DataFrame, df_clean: pd.DataFrame) -> float:
    """
    Calcula tasa de retenci√≥n de datos

    Args:
    -----------
    df_original : DataFrame original
    df_clean : DataFrame limpio

    Returns:
    --------
        Tasa de retenci√≥n en porcentaje
    """
    return (len(df_clean) / len(df_original)) * 100


def generate_summary_report(df_original: pd.DataFrame, df_final: pd.DataFrame) -> None:
    """
    Genera reporte resumen del proceso de limpieza

    ParametArgsers:
    -----------
    df_original : DataFrame original
    df_final : DataFrame final limpio
    """
    print_section("RESUMEN DEL PROCESO DE LIMPIEZA")

    print(f"\nüìä M√âTRICAS GENERALES")
    print(f"  ‚Ä¢ Registros iniciales: {len(df_original):,}")
    print(f"  ‚Ä¢ Registros finales: {len(df_final):,}")
    print(f"  ‚Ä¢ Registros eliminados: {len(df_original) - len(df_final):,}")
    print(f"  ‚Ä¢ Tasa de retenci√≥n: {calculate_retention_rate(df_original, df_final):.2f}%")

    print(f"\nüìä ESTRUCTURA")
    print(f"  ‚Ä¢ Columnas iniciales: {df_original.shape[1]}")
    print(f"  ‚Ä¢ Columnas finales: {df_final.shape[1]}")
    print(f"  ‚Ä¢ Columnas a√±adidas: {df_final.shape[1] - df_original.shape[1]}")

    print(f"\nüìä CALIDAD DE DATOS")
    print(f"  ‚Ä¢ Valores nulos iniciales: {df_original.isnull().sum().sum():,}")
    print(f"  ‚Ä¢ Valores nulos finales: {df_final.isnull().sum().sum():,}")
    print(f"  ‚Ä¢ Duplicados iniciales: {df_original.duplicated().sum():,}")
    print(f"  ‚Ä¢ Duplicados finales: {df_final.duplicated().sum():,}")
```

**[‚¨Ü back to top](#tabla-de-contenidos)**

---

<a id="checklist-final"></a>
## Checklist Final

### ‚úÖ Antes de Crear tu Pull Request

```markdown
## üìã Checklist de Calidad

### Estructura
- [ ] Notebook sigue la plantilla de secciones
- [ ] Todas las secciones tienen t√≠tulo en may√∫sculas
- [ ] Celdas markdown entre secciones de c√≥digo

### C√≥digo
- [ ] Nombres de variables seg√∫n est√°ndares (df, df_clean, etc.)
- [ ] Nombres de columnas en ingl√©s
- [ ] Funciones tienen docstrings
- [ ] C√≥digo comentado en espa√±ol
- [ ] Sin variables gen√©ricas (x, y, temp, data)

### An√°lisis
- [ ] An√°lisis exploratorio completo
- [ ] Detecci√≥n de duplicados
- [ ] An√°lisis de valores nulos
- [ ] Detecci√≥n de outliers
- [ ] Feature engineering implementado

### Visualizaciones
- [ ] Todas las gr√°ficas tienen t√≠tulo
- [ ] Ejes etiquetados correctamente
- [ ] Uso de paletas de colores est√°ndar
- [ ] Grid habilitado (alpha=0.3)

### Documentaci√≥n
- [ ] Celdas markdown explicativas
- [ ] Comentarios claros en c√≥digo complejo
- [ ] Resumen al final del notebook
- [ ] M√©tricas de retenci√≥n calculadas

### Resultados
- [ ] Dataset final sin nulos
- [ ] Dataset final sin duplicados
- [ ] Variables codificadas correctamente
- [ ] Variables escaladas (si aplica)
- [ ] CSV final generado

### Git
- [ ] Commits con mensajes descriptivos
- [ ] Solo tu notebook en tu rama
- [ ] No modificaste archivos de otros
- [ ] No modificaste data/raw/bmw_pricing_v3.csv
```

---

## üìû Contacto y Soporte

Si tienes dudas sobre estos est√°ndares:

1. **Revisar ejemplos** en este documento
2. **Consultar en el grupo** de WhatsApp
3. **Crear issue** en GitHub con tag `[STANDARDS]`
4. **Discutir en reuni√≥n** de equipo

---

## üîÑ Actualizaciones

Este documento puede ser actualizado por consenso del equipo.

**√öltima actualizaci√≥n:** Noviembre 2025  
**Versi√≥n:** 1.0

---

**[‚¨Ü back to top](#tabla-de-contenidos)**

<div align="center">

**¬°Sigamos estos est√°ndares para un c√≥digo limpio y profesional!**

üöó BMW Pricing Project - Data Engineering üöó

</div>